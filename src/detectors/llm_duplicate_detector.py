"""
åŸºäºå¤§æ¨¡å‹çš„é‡å¤æ£€æµ‹å™¨
ä½¿ç”¨LangChainå’ŒRunnableParallelè¿›è¡Œå¹¶è¡Œæ£€æµ‹
"""

import json
import os
import time
import logging
from typing import List, Dict, Optional
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnableLambda, RunnableParallel

from ..models.api_models import DuplicateOutput
from ..models.data_models import TextSegment, DocumentData
from ..utils.text_utils import extract_prefix_suffix

logger = logging.getLogger(__name__)


class LLMDuplicateDetector:
    """åŸºäºå¤§æ¨¡å‹çš„é‡å¤æ£€æµ‹å™¨ - ä½¿ç”¨RunnableParallelå¹¶è¡Œæ‰§è¡Œ"""
    
    def __init__(self):
        # ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹å
        llm_model_name = os.environ.get("LLM_MODEL_NAME", "qwen-plus")
        
        self.llm = ChatOpenAI(
            model=llm_model_name, # type:ignore
            temperature=0.1
        )
        
        # åˆ›å»ºæç¤ºæ¨¡æ¿
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", self._get_system_prompt()),
            ("human", self._get_human_prompt())
        ])
        
        # åˆ›å»ºç›´æ¥æ¯”è¾ƒçš„æç¤ºæ¨¡æ¿
        self.direct_prompt = ChatPromptTemplate.from_messages([
            ("system", self._get_direct_system_prompt()),
            ("human", self._get_direct_human_prompt())
        ])
        
        # åˆ›å»ºåŸºç¡€é“¾
        self.base_chain = self.prompt | self.llm
        self.direct_chain = self.direct_prompt | self.llm
    
    def _get_system_prompt(self) -> str:
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æç³»ç»Ÿã€‚ä½ çš„ä»»åŠ¡æ˜¯æ£€æµ‹ç»™å®šæ–‡æœ¬ç‰‡æ®µä¸­çš„ä¸‰ç§é—®é¢˜ç±»å‹ï¼Œå¹¶è¾“å‡ºJSONæ ¼å¼çš„ç»“æœã€‚

**æ£€æµ‹ç±»å‹ï¼š**
ç±»åˆ«1 - è¯­ä¹‰ç›¸ä¼¼/é‡å¤ï¼š
- å®Œå…¨ç›¸åŒçš„æ–‡æœ¬ï¼ˆé€å­—ç›¸åŒï¼‰
- é«˜åº¦ç›¸ä¼¼çš„æ–‡æœ¬ï¼ˆä»…æœ‰å¾®å°å·®å¼‚ï¼Œå¦‚æ ‡ç‚¹ã€ç©ºæ ¼ã€åŒä¹‰è¯æ›¿æ¢ï¼‰
- æ„æ€ç›¸åŒä½†è¡¨è¾¾ç•¥æœ‰ä¸åŒçš„æ–‡æœ¬
- ç»“æ„ç›¸ä¼¼ã€é€»è¾‘ç›¸åŒçš„æ–‡æœ¬

ç±»åˆ«2 - é”™è¯¯ä¸€è‡´ï¼š
- ç›¸åŒçš„é”™åˆ«å­—ï¼ˆå¦‚"ç™»å½•"éƒ½å†™æˆ"ç™»é™†"ï¼‰
- ç›¸åŒçš„ç”¨è¯é”™è¯¯ï¼ˆå¦‚ä¸“ä¸šæœ¯è¯­ä½¿ç”¨é”™è¯¯ä½†é”™è¯¯æ–¹å¼ä¸€è‡´ï¼‰
- ç›¸åŒçš„è¯­æ³•é”™è¯¯
- ç›¸åŒçš„æ ¼å¼é”™è¯¯

ç±»åˆ«3 - æŠ¥ä»·å¼‚å¸¸ï¼š
- æŠ•æ ‡æŠ¥ä»·å‘ˆç­‰å·®æ•°åˆ—è§„å¾‹ï¼ˆå¦‚100ä¸‡ã€200ä¸‡ã€300ä¸‡...ï¼‰
- æŠ•æ ‡æŠ¥ä»·å‘ˆç­‰æ¯”æ•°åˆ—è§„å¾‹ï¼ˆå¦‚100ä¸‡ã€200ä¸‡ã€400ä¸‡...ï¼‰
- æŠ¥ä»·æ•°å€¼è¿‡äºæ¥è¿‘æˆ–æœ‰æ˜æ˜¾è§„å¾‹æ€§
- æŠ¥ä»·ç»“æ„å¼‚å¸¸ç›¸ä¼¼

**è¯„åˆ†æ ‡å‡†ï¼š**
- 0.9-1.0: é—®é¢˜éå¸¸æ˜æ˜¾
- 0.7-0.9: é—®é¢˜è¾ƒæ˜æ˜¾ï¼Œéœ€è¦å…³æ³¨
- 0.5-0.7: ä¸­ç­‰ç¨‹åº¦é—®é¢˜ï¼Œéœ€è¦äººå·¥ç¡®è®¤
- 0.3-0.5: è½»å¾®é—®é¢˜
- 0.0-0.3: æ— é—®é¢˜

è¯·ä»”ç»†åˆ†ææ¯å¯¹æ–‡æœ¬ç‰‡æ®µï¼Œè¯†åˆ«é—®é¢˜ç±»å‹å¹¶ç»™å‡ºå‡†ç¡®çš„åˆ¤æ–­ï¼Œæ‰¾å‡ºæ‰€æœ‰è¯„åˆ†è¾¾åˆ°0.8åˆ†ä»¥ä¸Šçš„é—®é¢˜å†…å®¹ã€‚"""

    def _get_human_prompt(self) -> str:
        return """è¯·åˆ†æä»¥ä¸‹æ¥è‡ªä¸åŒæ–‡æ¡£çš„æ–‡æœ¬ç‰‡æ®µï¼Œæ£€æµ‹ä¸‰ç§é—®é¢˜ç±»å‹ï¼š

{text_segments}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼š
{{
  "is_duplicate": true/false,
  "duplicate_pairs": [
    {{
      "documentId1": æ•°å­—,
      "page1": æ•°å­—,
      "chunkId1": æ•°å­—,
      "content1": "æ–‡æœ¬å†…å®¹",
      "documentId2": æ•°å­—,
      "page2": æ•°å­—,
      "chunkId2": æ•°å­—,
      "content2": "æ–‡æœ¬å†…å®¹",
      "reason": "åˆ¤æ–­åŸå› ",
      "score": æ•°å­—,
      "category": æ•°å­—
    }}
  ],
  "explanation": "æ•´ä½“åˆ¤æ–­è¯´æ˜"
}}

æ³¨æ„ï¼š
1. åªæ¯”è¾ƒæ¥è‡ªä¸åŒæ–‡æ¡£çš„ç‰‡æ®µ
2. categoryå­—æ®µï¼š1-è¯­ä¹‰ç›¸ä¼¼ï¼Œ2-é”™è¯¯ä¸€è‡´ï¼Œ3-æŠ¥ä»·å¼‚å¸¸
3. å¦‚æœå‘ç°é—®é¢˜ï¼Œè¯·åœ¨duplicate_pairsä¸­è¯¦ç»†åˆ—å‡ºæ¯ä¸€å¯¹é—®é¢˜å†…å®¹
4. æ¯ä¸ªduplicate_pairå¿…é¡»åŒ…å«å®Œæ•´çš„documentId, page, chunkId, content, categoryä¿¡æ¯
5. explanationåº”è¯¥è¯¦ç»†è¯´æ˜åˆ¤æ–­ä¾æ®å’Œé—®é¢˜ç±»å‹
6. è¿”å›çš„å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—"""
    
    def _get_direct_system_prompt(self) -> str:
        """ç›´æ¥æ¯”è¾ƒä¸¤ä¸ªå®Œæ•´æ–‡æ¡£çš„ç³»ç»Ÿæç¤º"""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æç³»ç»Ÿã€‚ä½ å°†æ¥æ”¶åˆ°ä¸¤ä¸ªå®Œæ•´çš„æ–‡æ¡£ï¼Œéœ€è¦æ‰¾å‡ºå…¶ä¸­çš„ä¸‰ç§é—®é¢˜ç±»å‹ã€‚

**æ£€æµ‹ç±»å‹ï¼š**
ç±»åˆ«1 - è¯­ä¹‰ç›¸ä¼¼/é‡å¤ï¼š
- å®Œå…¨ç›¸åŒçš„æ–‡æœ¬æ®µè½æˆ–å¥å­
- é«˜åº¦ç›¸ä¼¼çš„æ–‡æœ¬ï¼ˆä»…æœ‰å¾®å°å·®å¼‚ï¼Œå¦‚æ ‡ç‚¹ã€ç©ºæ ¼ã€åŒä¹‰è¯æ›¿æ¢ï¼‰  
- æ„æ€ç›¸åŒä½†è¡¨è¾¾ç•¥æœ‰ä¸åŒçš„æ–‡æœ¬æ®µè½
- ç»“æ„ç›¸ä¼¼ã€é€»è¾‘ç›¸åŒçš„æ®µè½

ç±»åˆ«2 - é”™è¯¯ä¸€è‡´ï¼š
- ç›¸åŒçš„é”™åˆ«å­—ï¼ˆå¦‚"ç™»å½•"éƒ½å†™æˆ"ç™»é™†"ï¼‰
- ç›¸åŒçš„ç”¨è¯é”™è¯¯
- ç›¸åŒçš„è¯­æ³•é”™è¯¯
- ç›¸åŒçš„æ ¼å¼é”™è¯¯

ç±»åˆ«3 - æŠ¥ä»·å¼‚å¸¸ï¼š
- æŠ•æ ‡æŠ¥ä»·å‘ˆç­‰å·®æ•°åˆ—æˆ–ç­‰æ¯”æ•°åˆ—è§„å¾‹
- æŠ¥ä»·æ•°å€¼è¿‡äºæ¥è¿‘æˆ–æœ‰æ˜æ˜¾è§„å¾‹æ€§
- æŠ¥ä»·ç»“æ„å¼‚å¸¸ç›¸ä¼¼

**è¯„åˆ†æ ‡å‡†ï¼š**
- 0.9-1.0: é—®é¢˜éå¸¸æ˜æ˜¾
- 0.7-0.9: é—®é¢˜è¾ƒæ˜æ˜¾ï¼Œéœ€è¦å…³æ³¨
- 0.5-0.7: ä¸­ç­‰ç¨‹åº¦é—®é¢˜ï¼Œéœ€è¦äººå·¥ç¡®è®¤

è¯·ä»”ç»†åˆ†æä¸¤ä¸ªæ–‡æ¡£ï¼Œæ‰¾å‡ºæ‰€æœ‰è¯„åˆ†è¾¾åˆ°0.8åˆ†ä»¥ä¸Šçš„é—®é¢˜å†…å®¹ã€‚"""
    
    def _get_direct_human_prompt(self) -> str:
        """ç›´æ¥æ¯”è¾ƒä¸¤ä¸ªå®Œæ•´æ–‡æ¡£çš„äººç±»æç¤º"""
        return """è¯·åˆ†æä»¥ä¸‹ä¸¤ä¸ªå®Œæ•´æ–‡æ¡£ï¼Œæ‰¾å‡ºå…¶ä¸­çš„ä¸‰ç§é—®é¢˜ç±»å‹ï¼š

**æ–‡æ¡£1:**
{document1}

**æ–‡æ¡£2:**  
{document2}

è¯·å°†æ¯ä¸ªé—®é¢˜çš„æ–‡æœ¬ç‰‡æ®µæ ‡è®°å‡ºæ¥ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
  "is_duplicate": true/false,
  "duplicate_pairs": [
    {{
      "content1": "æ–‡æ¡£1ä¸­çš„é—®é¢˜ç‰‡æ®µ",
      "content2": "æ–‡æ¡£2ä¸­çš„é—®é¢˜ç‰‡æ®µ", 
      "reason": "é—®é¢˜è¯´æ˜",
      "score": æ•°å­—,
      "category": æ•°å­—
    }}
  ],
  "explanation": "æ•´ä½“åˆ†æè¯´æ˜"
}}

æ³¨æ„ï¼š
1. categoryå­—æ®µï¼š1-è¯­ä¹‰ç›¸ä¼¼ï¼Œ2-é”™è¯¯ä¸€è‡´ï¼Œ3-æŠ¥ä»·å¼‚å¸¸
2. å¯¹äºæ¯ä¸ªå‘ç°çš„é—®é¢˜ï¼Œè¯¦ç»†è¯´æ˜åŸå› 
3. è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—"""
    
    def detect_duplicates_parallel(self, clusters_dict: Dict[int, List[TextSegment]]) -> List[DuplicateOutput]:
        """å¹¶è¡Œæ£€æµ‹å¤šä¸ªèšç±»ä¸­çš„é‡å¤å†…å®¹"""
        if not clusters_dict:
            return []
        
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {len(clusters_dict)} ä¸ªèšç±»...")
        start_time = time.time()
        
        # ä¸ºæ¯ä¸ªèšç±»åˆ›å»ºå¤„ç†å‡½æ•°
        def create_cluster_processor(cluster_id: int, segments: List[TextSegment]):
            def process_cluster(input_data):
                logger.info(f"  ğŸ” å¤„ç†èšç±» {cluster_id} ({len(segments)} ä¸ªç‰‡æ®µ)")
                return self.detect_duplicates(segments)
            return RunnableLambda(process_cluster)
        
        # åˆ›å»ºå¹¶è¡Œå¤„ç†ç®¡é“
        parallel_tasks = {}
        for cluster_id, segments in clusters_dict.items():
            task_name = f"cluster_{cluster_id}"
            parallel_tasks[task_name] = create_cluster_processor(cluster_id, segments)
        
        # ä½¿ç”¨RunnableParallelæ‰§è¡Œ
        parallel_runner = RunnableParallel(parallel_tasks)
        
        try:
            # æ‰§è¡Œå¹¶è¡Œä»»åŠ¡
            results = parallel_runner.invoke(
                {}, 
                model_kwargs={
                    "response_format": {"type": "json_object"}
                }
            )

            # åˆå¹¶æ‰€æœ‰ç»“æœ
            all_duplicate_results = []
            for task_name, task_results in results.items():
                cluster_id = task_name.replace("cluster_", "")
                if task_results:
                    logger.info(f"    âœ… èšç±» {cluster_id} å‘ç° {len(task_results)} å¯¹é‡å¤å†…å®¹")
                    all_duplicate_results.extend(task_results)
                else:
                    logger.info(f"    âœ… èšç±» {cluster_id} æœªå‘ç°é‡å¤å†…å®¹")
            
            end_time = time.time()
            logger.info(f"âš¡ å¹¶è¡Œå¤„ç†å®Œæˆï¼Œè€—æ—¶ {end_time - start_time:.2f} ç§’")
            logger.info(f"ğŸ“Š æ€»å…±å‘ç° {len(all_duplicate_results)} å¯¹é‡å¤å†…å®¹")
            
            return all_duplicate_results
            
        except Exception as e:
            logger.error(f"âŒ å¹¶è¡Œå¤„ç†å¤±è´¥: {e}")
            # å›é€€åˆ°ä¸²è¡Œå¤„ç†
            return self._fallback_serial_processing(clusters_dict)
    
    def _fallback_serial_processing(self, clusters_dict: Dict[int, List[TextSegment]]) -> List[DuplicateOutput]:
        """å›é€€çš„ä¸²è¡Œå¤„ç†æ–¹æ¡ˆ"""
        logger.info("ğŸ”„ å›é€€åˆ°ä¸²è¡Œå¤„ç†...")
        all_results = []
        
        for cluster_id, segments in clusters_dict.items():
            logger.info(f"  ğŸ” ä¸²è¡Œå¤„ç†èšç±» {cluster_id}")
            results = self.detect_duplicates(segments)
            if results:
                all_results.extend(results)
        
        return all_results
    
    def detect_duplicates(self, segments: List[TextSegment]) -> List[DuplicateOutput]:
        """æ£€æµ‹æ–‡æœ¬ç‰‡æ®µä¸­çš„é‡å¤å†…å®¹ï¼Œè¿”å›ç»“æ„åŒ–ç»“æœ"""
        
        # æ ¼å¼åŒ–è¾“å…¥æ–‡æœ¬
        text_segments = self._format_segments(segments)
        
        # è°ƒç”¨å¤§æ¨¡å‹é“¾
        try:
            response = self.base_chain.invoke({"text_segments": text_segments})
            
            # è§£æå“åº”å†…å®¹
            result_dict = self._parse_response(response.content) # type:ignore
            
            if result_dict and result_dict.get("is_duplicate") and result_dict.get("duplicate_pairs"):
                # è½¬æ¢ä¸º DuplicateOutput å¯¹è±¡
                duplicate_outputs = []
                for pair in result_dict["duplicate_pairs"]:
                    try:
                        content1 = str(pair["content1"])
                        content2 = str(pair["content2"])
                        prefix1, suffix1 = extract_prefix_suffix(content1)
                        prefix2, suffix2 = extract_prefix_suffix(content2)
                        output = DuplicateOutput(
                            documentId1=int(pair["documentId1"]),
                            page1=int(pair["page1"]),
                            chunkId1=int(pair["chunkId1"]),
                            content1=content1,
                            prefix1=prefix1,
                            suffix1=suffix1,
                            documentId2=int(pair["documentId2"]),
                            page2=int(pair["page2"]),
                            chunkId2=int(pair["chunkId2"]),
                            content2=content2,
                            prefix2=prefix2,
                            suffix2=suffix2,
                            reason=str(pair["reason"]),
                            score=float(pair["score"]),
                            category=int(pair.get("category", 1))  # é»˜è®¤ä¸ºè¯­ä¹‰ç›¸ä¼¼
                        )
                        duplicate_outputs.append(output)
                    except (KeyError, ValueError, TypeError) as e:
                        logger.warning(f"è§£æé‡å¤å¯¹æ—¶å‡ºé”™: {e}")
                        continue
                        
                return duplicate_outputs
            else:
                return []
                
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            # ç›´æ¥è¿”å›ç©ºç»“æœ
            return []
    
    def _parse_response(self, response_content: str) -> Optional[Dict]:
        """è§£æ LLM å“åº”"""
        try:
            # æ¸…ç†å“åº”å†…å®¹
            content = response_content.strip()
            
            # å°è¯•æå– JSON
            import re
            
            # æŸ¥æ‰¾ JSON å†…å®¹ï¼ˆåŒ…æ‹¬å¤šè¡Œï¼‰
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                return json.loads(json_str)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°èŠ±æ‹¬å·ï¼Œå°è¯•æ•´ä¸ªå†…å®¹
                return json.loads(content)
                
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            logger.error(f"åŸå§‹å“åº”å†…å®¹: {response_content[:500]}...")
            return None
        except Exception as e:
            logger.error(f"å“åº”è§£æå‡ºé”™: {e}")
            return None
    
    def direct_document_comparison(self, document_data_list: List[DocumentData]) -> List[DuplicateOutput]:
        """ç›´æ¥æ¯”è¾ƒç­–ç•¥ï¼šç›´æ¥æ¯”è¾ƒå®Œæ•´æ–‡æ¡£å†…å®¹"""
        logger.info("ğŸ”„ å¯åŠ¨ç›´æ¥æ¯”è¾ƒç­–ç•¥ï¼šç›´æ¥æ¯”è¾ƒå®Œæ•´æ–‡æ¡£")
        
        if len(document_data_list) < 2:
            return []
        
        results = []
        
        # ä¸¤ä¸¤æ¯”è¾ƒæ‰€æœ‰æ–‡æ¡£
        for i, doc1 in enumerate(document_data_list):
            for j, doc2 in enumerate(document_data_list[i+1:], i+1):
                if doc1.document_id != doc2.document_id:  # ç¡®ä¿æ¯”è¾ƒä¸åŒæ–‡æ¡£
                    try:
                        # è°ƒç”¨ç›´æ¥æ¯”è¾ƒé“¾
                        response = self.direct_chain.invoke({
                            "document1": doc1.content,
                            "document2": doc2.content
                        }, 
                            model_kwargs={
                            "response_format": {"type": "json_object"}
                            }
                        )

                        # è§£æå“åº”
                        result_dict = self._parse_response(response.content) # type:ignore
                        if result_dict and result_dict.get("is_duplicate") and result_dict.get("duplicate_pairs"):
                            for pair in result_dict["duplicate_pairs"]:
                                try:
                                    # è·å–å†…å®¹åœ¨å„è‡ªæ–‡æ¡£ä¸­çš„ç²¾ç¡®é¡µé¢ä¿¡æ¯
                                    content1 = str(pair["content1"])
                                    content2 = str(pair["content2"])
                                    
                                    page1 = self._find_content_page(content1, doc1)
                                    page2 = self._find_content_page(content2, doc2)
                                    
                                    prefix1, suffix1 = extract_prefix_suffix(content1)
                                    prefix2, suffix2 = extract_prefix_suffix(content2)
                                    
                                    # åˆ›å»ºè¾“å‡ºç»“æœï¼Œä½¿ç”¨ç²¾ç¡®çš„é¡µé¢ä¿¡æ¯å’Œå›ºå®šçš„chunk_id
                                    output = DuplicateOutput(
                                        documentId1=doc1.document_id,
                                        page1=page1,  # ç²¾ç¡®çš„é¡µé¢ä¿¡æ¯
                                        chunkId1=0,   # å›ºå®šçš„chunk_idï¼Œè¡¨ç¤ºç›´æ¥æ¯”è¾ƒ
                                        content1=content1,
                                        prefix1=prefix1,
                                        suffix1=suffix1,
                                        documentId2=doc2.document_id,
                                        page2=page2,  # ç²¾ç¡®çš„é¡µé¢ä¿¡æ¯
                                        chunkId2=0,   # å›ºå®šçš„chunk_idï¼Œè¡¨ç¤ºç›´æ¥æ¯”è¾ƒ
                                        content2=content2,
                                        prefix2=prefix2,
                                        suffix2=suffix2,
                                        reason=str(pair['reason']),
                                        score=float(pair["score"]),
                                        category=int(pair.get("category", 1))  # é»˜è®¤ä¸ºè¯­ä¹‰ç›¸ä¼¼
                                    )

                                    results.append(output)
                                except (KeyError, ValueError, TypeError) as e:
                                    logger.warning(f"è§£æç›´æ¥æ¯”è¾ƒç»“æœæ—¶å‡ºé”™: {e}")
                                    continue
                    
                    except Exception as e:
                        logger.error(f"ç›´æ¥æ–‡æ¡£æ¯”è¾ƒå¤±è´¥: {e}")
                        continue
        
        logger.info(f"ğŸ” ç›´æ¥æ¯”è¾ƒå‘ç° {len(results)} å¯¹é‡å¤å†…å®¹")
        return results
    
    def _find_content_page(self, content: str, document_data: DocumentData) -> int:
        """åœ¨æ–‡æ¡£çš„é¡µé¢ä¸­æ‰¾åˆ°å†…å®¹æ‰€åœ¨çš„é¡µé¢"""
        content = content.strip()
        
        # éå†æ‰€æœ‰é¡µé¢ï¼Œæ‰¾åˆ°åŒ…å«è¯¥å†…å®¹çš„é¡µé¢
        for page_num, page_content in document_data.pages.items():
            if content in page_content:
                return page_num
        
        # å¦‚æœåœ¨å•ä¸ªé¡µé¢ä¸­æ‰¾ä¸åˆ°ï¼Œå¯èƒ½æ˜¯è·¨é¡µé¢çš„å†…å®¹ï¼Œè¿”å›ç¬¬ä¸€ä¸ªåŒ…å«éƒ¨åˆ†å†…å®¹çš„é¡µé¢
        for page_num, page_content in document_data.pages.items():
            # æ£€æŸ¥å†…å®¹çš„å‰åŠéƒ¨åˆ†æˆ–ååŠéƒ¨åˆ†æ˜¯å¦åœ¨æŸä¸ªé¡µé¢ä¸­
            content_length = len(content)
            content_start = content[:content_length // 2]
            content_end = content[content_length // 2:]
            
            if content_start in page_content or content_end in page_content:
                return page_num
        
        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªé¡µé¢
        return min(document_data.pages.keys()) if document_data.pages else 1
    
    def _simple_similarity(self, text1: str, text2: str) -> float:
        """ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—"""
        if text1 == text2:
            return 1.0
        
        # åŸºäºå­—ç¬¦çš„ç›¸ä¼¼åº¦
        longer = text1 if len(text1) > len(text2) else text2
        shorter = text2 if len(text1) > len(text2) else text1
        
        if len(longer) == 0:
            return 1.0
        
        # è®¡ç®—ç¼–è¾‘è·ç¦»çš„ç®€åŒ–ç‰ˆæœ¬
        matches = sum(1 for a, b in zip(shorter, longer) if a == b)
        return matches / len(longer)
    
    def _format_segments(self, segments: List[TextSegment]) -> str:
        """æ ¼å¼åŒ–æ–‡æœ¬ç‰‡æ®µç”¨äºè¾“å…¥"""
        formatted = []
        
        for i, segment in enumerate(segments):
            formatted.append(
                f"**ç‰‡æ®µ {i+1}** (æ–‡æ¡£ID: {segment.document_id}, é¡µç : {segment.page}, ç‰‡æ®µID: {segment.chunk_id}):\n"
                f"{segment.content}\n"
            )
        
        return "\n".join(formatted)
