"""
æ–‡æ¡£æŸ¥é‡æ ¸å¿ƒæœåŠ¡
æ•´åˆæ‰€æœ‰åŠŸèƒ½æ¨¡å—ï¼Œæä¾›ç»Ÿä¸€çš„æœåŠ¡æ¥å£
"""

import time
import logging
import threading
from typing import List, Dict
from langchain.schema.runnable import RunnableLambda, RunnableParallel

from ..models.api_models import DuplicateOutput
from ..models.data_models import DocumentData
from ..core.document_processor import DocumentProcessor
from ..core.clustering_manager import ClusteringManager
from ..detectors.llm_duplicate_detector import LLMDuplicateDetector
from ..validators.validation_manager import ValidationManager

logger = logging.getLogger(__name__)


class DocumentDeduplicationService:
    """æ–‡æ¡£æŸ¥é‡æœåŠ¡"""
    
    def __init__(self):
        self.processor = DocumentProcessor()
        self.clustering_manager = ClusteringManager()
        self.detector = LLMDuplicateDetector()
        self.validator = ValidationManager()
        self._execution_lock = threading.Lock()
        self._is_running = False
    
    def analyze_documents(self, json_input: List[Dict]) -> List[DuplicateOutput]:
        """åˆ†ææ–‡æ¡£é‡å¤å†…å®¹ - å¹¶è¡Œå¤„ç†åˆ†å‰²èšç±»æŸ¥é‡å’Œç›´æ¥æŸ¥é‡"""
        
        # æ£€æŸ¥æ˜¯å¦å·²åœ¨æ‰§è¡Œ
        if self._is_running:
            logger.warning("å·¥ä½œæµæ­£åœ¨æ‰§è¡Œä¸­ï¼Œè¯·ç­‰å¾…å®Œæˆ...")
            return []
        
        with self._execution_lock:
            if self._is_running:
                return []
            
            self._is_running = True
            execution_id = int(time.time() * 1000)
            logger.info(f"å¼€å§‹æ‰§è¡Œå·¥ä½œæµ (ID: {execution_id})")
            
            try:
                # 1. å¤„ç†è¾“å…¥æ•°æ®
                logger.info(f"[{execution_id}] æ­£åœ¨å¤„ç†JSONè¾“å…¥...")
                document_data_list, document_inputs = self.processor.process_json_documents(json_input)
                
                # 2. å¹¶è¡Œæ‰§è¡Œä¸¤ç§ç­–ç•¥
                logger.info(f"[{execution_id}] ğŸš€ å¼€å§‹å¹¶è¡Œæ‰§è¡Œåˆ†å‰²èšç±»æŸ¥é‡å’Œç›´æ¥æŸ¥é‡...")
                
                def clustering_strategy():
                    """åˆ†å‰²èšç±»æŸ¥é‡ç­–ç•¥"""
                    try:
                        # åˆ†å‰²æ–‡æ¡£
                        segments = self.processor.segment_documents(document_inputs)
                        logger.info(f"[{execution_id}] èšç±»ç­–ç•¥ï¼šå·²åˆ†å‰²å‡º {len(segments)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
                        
                        # ç”ŸæˆåµŒå…¥å‘é‡
                        segments = self.processor.generate_embeddings(segments)
                        logger.info(f"[{execution_id}] èšç±»ç­–ç•¥ï¼šå·²ç”Ÿæˆ {len(segments)} ä¸ªåµŒå…¥å‘é‡")
                        
                        # èšç±»åˆ†æ
                        clusters = self.clustering_manager.initial_clustering(segments)
                        multi_doc_clusters = self.clustering_manager.filter_multi_document_clusters(clusters)
                        logger.info(f"[{execution_id}] èšç±»ç­–ç•¥ï¼šå‘ç° {len(multi_doc_clusters)} ä¸ªå¯èƒ½åŒ…å«é‡å¤å†…å®¹çš„èšç±»")
                        
                        # æ£€æµ‹é‡å¤å†…å®¹
                        if multi_doc_clusters:
                            cluster_results = self.detector.detect_duplicates_parallel(multi_doc_clusters)
                        else:
                            cluster_results = []
                        
                        logger.info(f"[{execution_id}] èšç±»ç­–ç•¥ï¼šå‘ç° {len(cluster_results)} å¯¹é‡å¤å†…å®¹")
                        return cluster_results
                        
                    except Exception as e:
                        logger.error(f"[{execution_id}] èšç±»ç­–ç•¥å¤±è´¥: {e}")
                        return []
                
                def direct_strategy():
                    """ç›´æ¥æŸ¥é‡ç­–ç•¥"""
                    try:
                        direct_results = self.detector.direct_document_comparison(document_data_list)
                        logger.info(f"[{execution_id}] ç›´æ¥ç­–ç•¥ï¼šå‘ç° {len(direct_results)} å¯¹é‡å¤å†…å®¹")
                        return direct_results
                        
                    except Exception as e:
                        logger.error(f"[{execution_id}] ç›´æ¥ç­–ç•¥å¤±è´¥: {e}")
                        return []
                
                # ä½¿ç”¨RunnableParallelå¹¶è¡Œæ‰§è¡Œä¸¤ç§ç­–ç•¥
                parallel_tasks = {
                    "clustering": RunnableLambda(lambda x: clustering_strategy()),
                    "direct": RunnableLambda(lambda x: direct_strategy())
                }
                
                parallel_runner = RunnableParallel(parallel_tasks)
                
                try:
                    results = parallel_runner.invoke({})
                    cluster_results = results.get("clustering", [])
                    direct_results = results.get("direct", [])
                except Exception as e:
                    logger.error(f"[{execution_id}] å¹¶è¡Œæ‰§è¡Œå¤±è´¥ï¼Œå›é€€åˆ°ä¸²è¡Œ: {e}")
                    cluster_results = clustering_strategy()
                    direct_results = direct_strategy()
                
                # 3. åˆå¹¶å¹¶å»é‡ç»“æœ
                logger.info(f"[{execution_id}] åˆå¹¶ç»“æœï¼šèšç±» {len(cluster_results)} + ç›´æ¥ {len(direct_results)}")
                combined_results = cluster_results + direct_results
                unique_results = self._deduplicate_results(combined_results)
                
                # 4. éªŒè¯ç»“æœ
                if unique_results:
                    logger.info(f"[{execution_id}] ğŸ” å¼€å§‹éªŒè¯æ£€æµ‹ç»“æœ...")
                    validated_results = self.validator.validate_results(document_data_list, unique_results)
                    logger.info(f"[{execution_id}] éªŒè¯å®Œæˆï¼Œæœ€ç»ˆç»“æœ: {len(validated_results)} å¯¹é‡å¤å†…å®¹")
                else:
                    validated_results = []
                
                return validated_results
                
            except Exception as e:
                logger.error(f"[{execution_id}] æ–‡æ¡£åˆ†æå¤±è´¥: {e}")
                raise
            finally:
                self._is_running = False
    
    def _deduplicate_results(self, results: List[DuplicateOutput]) -> List[DuplicateOutput]:
        """å»é™¤é‡å¤çš„æ£€æµ‹ç»“æœ"""
        if not results:
            return results
        
        unique_results = []
        seen_pairs = set()
        
        for result in results:
            # åˆ›å»ºæ ‡å‡†åŒ–çš„å†…å®¹å¯¹æ ‡è¯†
            content_pair = tuple(sorted([
                result.content1.strip().lower(),
                result.content2.strip().lower()
            ]))
            
            if content_pair not in seen_pairs:
                seen_pairs.add(content_pair)
                unique_results.append(result)
        
        logger.info(f"å»é‡å‰: {len(results)} å¯¹ï¼Œå»é‡å: {len(unique_results)} å¯¹")
        return unique_results
