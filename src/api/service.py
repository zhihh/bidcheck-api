"""
æ–‡æ¡£æŸ¥é‡æ ¸å¿ƒæœåŠ¡ - é«˜å¹¶å‘ç‰ˆæœ¬
æ•´åˆæ‰€æœ‰åŠŸèƒ½æ¨¡å—ï¼Œæä¾›ç»Ÿä¸€çš„æœåŠ¡æ¥å£ï¼Œæ”¯æŒå¹¶å‘å¤„ç†
"""

import time
import logging
import asyncio
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.schema.runnable import RunnableLambda, RunnableParallel

from ..models.api_models import DuplicateOutput
from ..models.data_models import DocumentData
from ..core.document_processor import DocumentProcessor
from ..core.clustering_manager import ClusteringManager
from ..detectors.llm_duplicate_detector import LLMDuplicateDetector
from ..validators.validation_manager import ValidationManager

logger = logging.getLogger(__name__)


class DocumentDeduplicationService:
    """æ–‡æ¡£æŸ¥é‡æœåŠ¡ - é«˜å¹¶å‘ç‰ˆæœ¬"""
    
    def __init__(self):
        self.processor = DocumentProcessor()
        self.clustering_manager = ClusteringManager()
        self.detector = LLMDuplicateDetector()
        self.validator = ValidationManager()
        # ç§»é™¤å…¨å±€é”ï¼Œæ”¯æŒå¹¶å‘å¤„ç†
        self.max_workers = 4  # å¯æ ¹æ®æœåŠ¡å™¨é…ç½®è°ƒæ•´
    
    async def analyze_documents(self, json_input: List[Dict]) -> List[DuplicateOutput]:
        """åˆ†ææ–‡æ¡£é‡å¤å†…å®¹ - é«˜å¹¶å‘å¼‚æ­¥å¤„ç†ç‰ˆæœ¬"""
        
        execution_id = int(time.time() * 1000)
        logger.info(f"å¼€å§‹æ‰§è¡Œå·¥ä½œæµ (ID: {execution_id})")
        
        try:
            # 1. å¤„ç†è¾“å…¥æ•°æ®
            logger.info(f"[{execution_id}] æ­£åœ¨å¤„ç†JSONè¾“å…¥...")
            document_data_list, document_inputs = await self._run_in_executor(
                self.processor.process_json_documents, json_input
            )
            
            # 2. å¹¶è¡Œæ‰§è¡Œä¸¤ç§ç­–ç•¥
            logger.info(f"[{execution_id}] ğŸš€ å¼€å§‹å¹¶è¡Œæ‰§è¡Œåˆ†å‰²èšç±»æŸ¥é‡å’Œç›´æ¥æŸ¥é‡...")
            
            # ä½¿ç”¨asyncioåˆ›å»ºå¹¶å‘ä»»åŠ¡
            cluster_task = asyncio.create_task(
                self._clustering_strategy(execution_id, document_inputs)
            )
            direct_task = asyncio.create_task(
                self._direct_strategy(execution_id, document_data_list)
            )
            
            # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡å®Œæˆ
            cluster_results, direct_results = await asyncio.gather(
                cluster_task, 
                direct_task, 
                return_exceptions=True
            )
            
            # å¤„ç†å¼‚å¸¸ç»“æœ
            if isinstance(cluster_results, Exception):
                logger.error(f"[{execution_id}] èšç±»ç­–ç•¥å¤±è´¥: {cluster_results}")
                cluster_results = []
            
            if isinstance(direct_results, Exception):
                logger.error(f"[{execution_id}] ç›´æ¥ç­–ç•¥å¤±è´¥: {direct_results}")
                direct_results = []
            
            # ç¡®ä¿ç»“æœæ˜¯åˆ—è¡¨ç±»å‹
            cluster_results = cluster_results if isinstance(cluster_results, list) else []
            direct_results = direct_results if isinstance(direct_results, list) else []
            
            # 3. åˆå¹¶å¹¶å»é‡ç»“æœ
            logger.info(f"[{execution_id}] åˆå¹¶ç»“æœï¼šèšç±» {len(cluster_results)} + ç›´æ¥ {len(direct_results)}")
            combined_results = cluster_results + direct_results
            unique_results = self._deduplicate_results(combined_results)
            
            # 4. éªŒè¯ç»“æœ
            if unique_results:
                logger.info(f"[{execution_id}] ğŸ” å¼€å§‹éªŒè¯æ£€æµ‹ç»“æœ...")
                validated_results = await self._run_in_executor(
                    self.validator.validate_results, document_data_list, unique_results
                )
                logger.info(f"[{execution_id}] éªŒè¯å®Œæˆï¼Œæœ€ç»ˆç»“æœ: {len(validated_results)} å¯¹é‡å¤å†…å®¹")
            else:
                validated_results = []
            
            return validated_results
            
        except Exception as e:
            logger.error(f"[{execution_id}] æ–‡æ¡£åˆ†æå¤±è´¥: {e}")
            raise
    
    async def _clustering_strategy(self, execution_id: int, document_inputs) -> List[DuplicateOutput]:
        """åˆ†å‰²èšç±»æŸ¥é‡ç­–ç•¥ - å¼‚æ­¥ç‰ˆæœ¬"""
        try:
            # åˆ†å‰²æ–‡æ¡£
            segments = await self._run_in_executor(
                self.processor.segment_documents, document_inputs
            )
            logger.info(f"[{execution_id}] èšç±»ç­–ç•¥ï¼šå·²åˆ†å‰²å‡º {len(segments)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            segments = await self._run_in_executor(
                self.processor.generate_embeddings, segments
            )
            logger.info(f"[{execution_id}] èšç±»ç­–ç•¥ï¼šå·²ç”Ÿæˆ {len(segments)} ä¸ªåµŒå…¥å‘é‡")
            
            # èšç±»åˆ†æ
            clusters = await self._run_in_executor(
                self.clustering_manager.initial_clustering, segments
            )
            multi_doc_clusters = await self._run_in_executor(
                self.clustering_manager.filter_multi_document_clusters, clusters
            )
            logger.info(f"[{execution_id}] èšç±»ç­–ç•¥ï¼šå‘ç° {len(multi_doc_clusters)} ä¸ªå¯èƒ½åŒ…å«é‡å¤å†…å®¹çš„èšç±»")
            
            # æ£€æµ‹é‡å¤å†…å®¹
            if multi_doc_clusters:
                cluster_results = await self._run_in_executor(
                    self.detector.detect_duplicates_parallel, multi_doc_clusters
                )
            else:
                cluster_results = []
            
            logger.info(f"[{execution_id}] èšç±»ç­–ç•¥ï¼šå‘ç° {len(cluster_results)} å¯¹é‡å¤å†…å®¹")
            return cluster_results
            
        except Exception as e:
            logger.error(f"[{execution_id}] èšç±»ç­–ç•¥å¤±è´¥: {e}")
            return []
    
    async def _direct_strategy(self, execution_id: int, document_data_list) -> List[DuplicateOutput]:
        """ç›´æ¥æŸ¥é‡ç­–ç•¥ - å¼‚æ­¥ç‰ˆæœ¬"""
        try:
            direct_results = await self._run_in_executor(
                self.detector.direct_document_comparison, document_data_list
            )
            logger.info(f"[{execution_id}] ç›´æ¥ç­–ç•¥ï¼šå‘ç° {len(direct_results)} å¯¹é‡å¤å†…å®¹")
            return direct_results
            
        except Exception as e:
            logger.error(f"[{execution_id}] ç›´æ¥ç­–ç•¥å¤±è´¥: {e}")
            return []
    
    async def _run_in_executor(self, func, *args):
        """åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥å‡½æ•°"""
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            return await loop.run_in_executor(executor, func, *args)
    
    def _deduplicate_results(self, results: List[DuplicateOutput]) -> List[DuplicateOutput]:
        """å»é™¤é‡å¤çš„æ£€æµ‹ç»“æœ"""
        if not results:
            return results
        
        unique_results = []
        seen_pairs = set()
        
        for result in results:
            # åˆ›å»ºæ ‡å‡†åŒ–çš„å†…å®¹å¯¹æ ‡è¯†
            content_pair = tuple(sorted([
                result.content1.strip().lower(),
                result.content2.strip().lower()
            ]))
            
            if content_pair not in seen_pairs:
                seen_pairs.add(content_pair)
                unique_results.append(result)
        
        logger.info(f"å»é‡å‰: {len(results)} å¯¹ï¼Œå»é‡å: {len(unique_results)} å¯¹")
        return unique_results
