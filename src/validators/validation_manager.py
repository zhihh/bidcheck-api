"""
éªŒè¯ç®¡ç†å™¨
åŸºäºè§„åˆ™çš„æº¯æºéªŒè¯é€»è¾‘
"""

import logging
from typing import List, Dict, Any

from ..models.api_models import DuplicateOutput
from ..models.data_models import DocumentData

logger = logging.getLogger(__name__)


class ValidationManager:
    """éªŒè¯ç®¡ç†å™¨ - åŸºäºè§„åˆ™çš„æº¯æºéªŒè¯"""
    
    def __init__(self):
        pass
    
    def validate_results(self, document_data_list: List[DocumentData], 
                        detected_results: List[DuplicateOutput]) -> List[DuplicateOutput]:
        """åŸºäºè§„åˆ™çš„éªŒè¯æ£€æµ‹ç»“æœ"""
        if not detected_results:
            return detected_results
        
        logger.info("ğŸ” å¼€å§‹éªŒè¯æ£€æµ‹ç»“æœ...")
        
        # åŸºäºè§„åˆ™çš„éªŒè¯
        rule_validated = self._rule_based_validation(document_data_list, detected_results)
        
        logger.info(f"âœ… éªŒè¯å®Œæˆï¼Œä¿ç•™ {len(rule_validated)} å¯¹é‡å¤å†…å®¹")
        return rule_validated
    
    def _rule_based_validation(self, document_data_list: List[DocumentData], 
                              results: List[DuplicateOutput]) -> List[DuplicateOutput]:
        """åŸºäºè§„åˆ™çš„éªŒè¯ - æ£€æŸ¥contentå­—æ®µæ˜¯å¦æ¥è‡ªåŸæ–‡è€Œä¸æ˜¯å¤§æ¨¡å‹çš„å¹»è§‰"""
        logger.info("ğŸ“ å¼€å§‹åŸºäºè§„åˆ™çš„æº¯æºéªŒè¯...")
        
        # åˆ›å»ºæ–‡æ¡£å†…å®¹æŸ¥æ‰¾å­—å…¸
        doc_dict = {doc.document_id: doc.content for doc in document_data_list}
        
        validated_results = []
        
        for result in results:
            # è·å–åŸæ–‡æ¡£å†…å®¹
            doc1_content = doc_dict.get(result.documentId1, "")
            doc2_content = doc_dict.get(result.documentId2, "")
            
            if not doc1_content or not doc2_content:
                logger.warning(f"éªŒè¯å¤±è´¥ï¼šæ‰¾ä¸åˆ°æ–‡æ¡£å†…å®¹ (doc1: {result.documentId1}, doc2: {result.documentId2})")
                continue
            
            # æº¯æºéªŒè¯ï¼šæ£€æŸ¥contentå­—æ®µåœ¨åŸæ–‡ä¸­çš„ä½ç½®å¹¶è®¡ç®—åŒ¹é…åº¦
            content1_match = self._find_best_match_in_source(result.content1, doc1_content)
            content2_match = self._find_best_match_in_source(result.content2, doc2_content)
            
            # è®¾å®šç›¸ä¼¼åº¦é˜ˆå€¼
            similarity_threshold = 0.7
            
            if content1_match['similarity'] < similarity_threshold or content2_match['similarity'] < similarity_threshold:
                logger.warning(f"æº¯æºéªŒè¯å¤±è´¥ï¼šcontentä¸åŸæ–‡åŒ¹é…åº¦ä¸è¶³ (content1: {content1_match['similarity']:.3f}, content2: {content2_match['similarity']:.3f})")
                continue
            
            # éªŒè¯é€šè¿‡ï¼šç”¨åŸæ–‡ä¸­å¯¹åº”çš„å†…å®¹æ›¿æ¢æ£€æµ‹ç»“æœä¸­çš„content
            validated_result = DuplicateOutput(
                documentId1=result.documentId1,
                page1=result.page1,
                chunkId1=result.chunkId1,
                content1=content1_match['matched_text'],  # ä½¿ç”¨åŸæ–‡ä¸­çš„å†…å®¹
                documentId2=result.documentId2,
                page2=result.page2,
                chunkId2=result.chunkId2,
                content2=content2_match['matched_text'],  # ä½¿ç”¨åŸæ–‡ä¸­çš„å†…å®¹
                reason=result.reason,
                score=result.score
            )
            
            # é¢å¤–éªŒè¯ï¼šæ£€æŸ¥å†…å®¹é•¿åº¦åˆç†æ€§
            if len(validated_result.content1.strip()) < 10 or len(validated_result.content2.strip()) < 10:
                logger.warning(f"éªŒè¯å¤±è´¥ï¼šå†…å®¹è¿‡çŸ­")
                continue
            
            # é¢å¤–éªŒè¯ï¼šæ£€æŸ¥ç›¸ä¼¼åº¦åˆ†æ•°åˆç†æ€§
            if result.score < 0.3 or result.score > 1.0:
                logger.warning(f"éªŒè¯å¤±è´¥ï¼šç›¸ä¼¼åº¦åˆ†æ•°ä¸åˆç† ({result.score})")
                continue
            
            validated_results.append(validated_result)
        
        logger.info(f"ğŸ“ è§„åˆ™éªŒè¯å®Œæˆï¼Œä¿ç•™ {len(validated_results)}/{len(results)} å¯¹ç»“æœ")
        return validated_results
    
    def _find_best_match_in_source(self, content: str, source_document: str) -> Dict[str, Any]:
        """åœ¨æºæ–‡æ¡£ä¸­æ‰¾åˆ°ä¸contentæœ€åŒ¹é…çš„ç‰‡æ®µ"""
        content = content.strip()
        
        # æ–¹æ³•1ï¼šç²¾ç¡®åŒ¹é…
        if content in source_document:
            return {
                'matched_text': content,
                'similarity': 1.0,
                'method': 'exact_match'
            }
        
        # æ–¹æ³•2ï¼šå­å­—ç¬¦ä¸²åŒ¹é… 
        best_match = {
            'matched_text': content,
            'similarity': 0.0,
            'method': 'substring_match'
        }
        
        # å°†æ–‡æ¡£åˆ†å‰²æˆå¥å­è¿›è¡ŒåŒ¹é…
        import re
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]+', source_document)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 5]
        
        # æŸ¥æ‰¾æœ€ä½³åŒ¹é…çš„å¥å­
        for sentence in sentences:
            similarity = self._calculate_string_similarity(content, sentence)
            if similarity > best_match['similarity']:
                best_match = {
                    'matched_text': sentence,
                    'similarity': similarity,
                    'method': 'sentence_match'
                }
        
        # æ–¹æ³•3ï¼šæ»‘åŠ¨çª—å£åŒ¹é…
        window_size = len(content)
        for i in range(0, len(source_document) - window_size + 1, 10):  # æ­¥é•¿ä¸º10
            window_text = source_document[i:i + window_size]
            similarity = self._calculate_string_similarity(content, window_text)
            if similarity > best_match['similarity']:
                best_match = {
                    'matched_text': window_text,
                    'similarity': similarity,
                    'method': 'window_match'
                }
        
        return best_match
    
    def _calculate_string_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦"""
        if not text1 or not text2:
            return 0.0
        
        if text1 == text2:
            return 1.0
        
        # ä½¿ç”¨ç¼–è¾‘è·ç¦»è®¡ç®—ç›¸ä¼¼åº¦
        def levenshtein_distance(s1: str, s2: str) -> int:
            if len(s1) < len(s2):
                return levenshtein_distance(s2, s1)
            
            if len(s2) == 0:
                return len(s1)
            
            previous_row = list(range(len(s2) + 1))
            for i, c1 in enumerate(s1):
                current_row = [i + 1]
                for j, c2 in enumerate(s2):
                    insertions = previous_row[j + 1] + 1
                    deletions = current_row[j] + 1
                    substitutions = previous_row[j] + (c1 != c2)
                    current_row.append(min(insertions, deletions, substitutions))
                previous_row = current_row
            
            return previous_row[-1]
        
        max_len = max(len(text1), len(text2))
        distance = levenshtein_distance(text1, text2)
        similarity = 1 - (distance / max_len)
        
        return max(0.0, similarity)
    
    def _calculate_simple_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦"""
        if text1 == text2:
            return 1.0
        
        # åŸºäºå­—ç¬¦çš„ç›¸ä¼¼åº¦
        longer = text1 if len(text1) > len(text2) else text2
        shorter = text2 if len(text1) > len(text2) else text1
        
        if len(longer) == 0:
            return 1.0
        
        # è®¡ç®—ç¼–è¾‘è·ç¦»çš„ç®€åŒ–ç‰ˆæœ¬
        matches = sum(1 for a, b in zip(shorter, longer) if a == b)
        return matches / len(longer)
