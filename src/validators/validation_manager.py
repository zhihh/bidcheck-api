"""
éªŒè¯ç®¡ç†å™¨
è´Ÿè´£éªŒè¯æ£€æµ‹ç»“æœçš„å‡†ç¡®æ€§å’Œæº¯æºæ€§
ä½¿ç”¨ä¼˜åŒ–ç®—æ³•ï¼šBoyer-Mooreå­—ç¬¦ä¸²æŸ¥æ‰¾ã€Levenshteinç¼–è¾‘è·ç¦»ã€å‘é‡ç›¸ä¼¼åº¦éªŒè¯
æ”¯æŒGPUåŠ é€Ÿå’Œå¤šçº¿ç¨‹å¤„ç†
"""

import logging
import os
import re
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Tuple, Optional
import numpy as np

try:
    import cupy as cp
    CUPY_AVAILABLE = True
except ImportError:
    CUPY_AVAILABLE = False
    cp = None

try:
    from numba import jit
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False
    jit = lambda func: func

from ..models.api_models import DuplicateOutput
from ..models.data_models import DocumentData
from ..utils.text_utils import extract_prefix_suffix
from ..config.config import Config

logger = logging.getLogger(__name__)


class ValidationManager:
    """éªŒè¯ç®¡ç†å™¨ - ä½¿ç”¨ä¼˜åŒ–ç®—æ³•è¿›è¡Œé«˜æ•ˆéªŒè¯"""
    
    def __init__(self, max_workers: Optional[int] = None, use_gpu: bool = True, 
                 similarity_threshold: Optional[float] = None, vector_threshold: Optional[float] = None):
        """
        åˆå§‹åŒ–éªŒè¯ç®¡ç†å™¨
        
        Args:
            max_workers: æœ€å¤§çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
            use_gpu: æ˜¯å¦å°è¯•ä½¿ç”¨GPUåŠ é€Ÿ
            similarity_threshold: å­—ç¬¦ä¸²ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
            vector_threshold: å‘é‡ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
        """
        self.config = Config()
        
        # ä»é…ç½®æ–‡ä»¶æˆ–å‚æ•°è·å–è®¾ç½®
        self.max_workers = max_workers or self.config.validation_max_workers
        self.use_gpu = use_gpu and self.config.validation_use_gpu and CUPY_AVAILABLE
        self.similarity_threshold = similarity_threshold or self.config.validation_similarity_threshold
        self.vector_threshold = vector_threshold or self.config.validation_vector_threshold
        
        # åˆå§‹åŒ–GPUè®¾å¤‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_gpu and cp is not None:
            try:
                cp.cuda.Device(0).use()
                logger.info("ï¿½ GPUåŠ é€Ÿå·²å¯ç”¨ (CuPy)")
            except Exception as e:
                logger.warning(f"GPUåˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
                self.use_gpu = False
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹ï¼ˆç”¨äºå‘é‡ç›¸ä¼¼åº¦ï¼‰
        self._init_embedding_model()
    
    def _init_embedding_model(self):
        """åˆå§‹åŒ–embeddingæ¨¡å‹"""
        try:
            # ä½¿ç”¨ä¸document_processorç›¸åŒçš„OpenAI clientæ–¹å¼
            from openai import OpenAI
            
            if self.config.openai_api_key:
                os.environ['OPENAI_API_KEY'] = self.config.openai_api_key
            
            self.client = OpenAI(
                api_key=self.config.openai_api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
            self.embedding_model_name = self.config.embedding_model_name
            logger.info("ğŸ“Š OpenAIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–")
        except Exception as e:
            logger.warning(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.client = None
            self.embedding_model_name = None
    
    def boyer_moore_search(self, pattern: str, text: str) -> List[int]:
        """
        Boyer-Mooreå­—ç¬¦ä¸²æœç´¢ç®—æ³• - é«˜æ•ˆç²¾ç¡®åŒ¹é…
        å®Œæ•´ç‰ˆå®ç°ï¼ˆåå­—ç¬¦è§„åˆ™ + å¥½åç¼€è§„åˆ™ï¼‰
        
        Args:
            pattern: è¦æœç´¢çš„æ¨¡å¼å­—ç¬¦ä¸²
            text: ç›®æ ‡æ–‡æœ¬
            
        Returns:
            List[int]: æ‰€æœ‰åŒ¹é…ä½ç½®çš„åˆ—è¡¨
        """
        if not pattern or not text:
            return []
        
        # æ„å»ºåå­—ç¬¦è¡¨
        def build_bad_char_table(pattern: str) -> Dict[str, int]:
            table = {}
            for i, char in enumerate(pattern):
                table[char] = i
            return table
        
        # æ„å»ºå¥½åç¼€è¡¨ï¼ˆç®€åŒ–ä½†æœ‰æ•ˆçš„ç‰ˆæœ¬ï¼‰
        def build_good_suffix_table(pattern: str) -> List[int]:
            m = len(pattern)
            table = [m] * m  # é»˜è®¤è·³è·ƒæ•´ä¸ªæ¨¡å¼é•¿åº¦
            
            # è®¡ç®—è¾¹ç•Œæ•°ç»„
            border = [0] * m
            i, j = m - 1, m
            border[i] = j
            
            while i > 0:
                while j < m and pattern[i] != pattern[j]:
                    if table[j] == m:
                        table[j] = j - i
                    j = border[j]
                i -= 1
                j -= 1
                border[i] = j
            
            # å¤„ç†å‰ç¼€æƒ…å†µ
            j = border[0]
            for i in range(m):
                if table[i] == m:
                    table[i] = j
                if i == j:
                    j = border[j]
            
            return table
        
        bad_char_table = build_bad_char_table(pattern)
        good_suffix_table = build_good_suffix_table(pattern)
        
        matches = []
        m, n = len(pattern), len(text)
        i = 0
        
        while i <= n - m:
            j = m - 1
            # ä»æ¨¡å¼ä¸²æœ«å°¾å¼€å§‹æ¯”è¾ƒ
            while j >= 0 and pattern[j] == text[i + j]:
                j -= 1
            
            if j < 0:
                # æ‰¾åˆ°åŒ¹é…
                matches.append(i)
                # ä½¿ç”¨å¥½åç¼€è¡¨å†³å®šè·³è·ƒè·ç¦»
                i += good_suffix_table[0]
            else:
                # è®¡ç®—åå­—ç¬¦å’Œå¥½åç¼€çš„è·³è·ƒè·ç¦»ï¼Œå–è¾ƒå¤§è€…
                bad_char_shift = max(1, j - bad_char_table.get(text[i + j], -1))
                good_suffix_shift = good_suffix_table[j]
                i += max(bad_char_shift, good_suffix_shift)
        
        return matches
    
    def _is_prefix(self, pattern: str, p: int) -> bool:
        """æ£€æŸ¥pattern[p:]æ˜¯å¦ä¸ºpatternçš„å‰ç¼€"""
        j = 0
        for i in range(p, len(pattern)):
            if pattern[i] != pattern[j]:
                return False
            j += 1
        return True
    
    def _suffix_length(self, pattern: str, p: int) -> int:
        """è®¡ç®—pattern[0:p+1]ä¸patternçš„åç¼€çš„æœ€é•¿å…¬å…±é•¿åº¦"""
        length = 0
        j = len(pattern) - 1
        i = p
        while i >= 0 and pattern[i] == pattern[j]:
            length += 1
            i -= 1
            j -= 1
        return length
    
    @jit
    def levenshtein_distance_optimized(self, s1: str, s2: str) -> int:
        """
        ä¼˜åŒ–çš„Levenshteinç¼–è¾‘è·ç¦»ç®—æ³•
        ä½¿ç”¨æ»šåŠ¨æ•°ç»„ä¼˜åŒ–ç©ºé—´å¤æ‚åº¦ï¼Œæ”¯æŒJITç¼–è¯‘åŠ é€Ÿ
        
        Args:
            s1, s2: è¦æ¯”è¾ƒçš„ä¸¤ä¸ªå­—ç¬¦ä¸²
            
        Returns:
            int: ç¼–è¾‘è·ç¦»
        """
        if not s1:
            return len(s2)
        if not s2:
            return len(s1)
        
        # ä½¿ç”¨æ»šåŠ¨æ•°ç»„ï¼Œç©ºé—´å¤æ‚åº¦O(min(m,n))
        if len(s1) > len(s2):
            s1, s2 = s2, s1
        
        m, n = len(s1), len(s2)
        prev_row = list(range(m + 1))
        
        for i in range(1, n + 1):
            curr_row = [i]
            for j in range(1, m + 1):
                cost = 0 if s1[j-1] == s2[i-1] else 1
                curr_row.append(min(
                    curr_row[j-1] + 1,      # æ’å…¥
                    prev_row[j] + 1,        # åˆ é™¤
                    prev_row[j-1] + cost    # æ›¿æ¢
                ))
            prev_row = curr_row
        
        return prev_row[m]
    
    def levenshtein_similarity(self, s1: str, s2: str) -> float:
        """
        åŸºäºLevenshteinè·ç¦»è®¡ç®—ç›¸ä¼¼åº¦
        
        Args:
            s1, s2: è¦æ¯”è¾ƒçš„ä¸¤ä¸ªå­—ç¬¦ä¸²
            
        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        if not s1 and not s2:
            return 1.0
        if not s1 or not s2:
            return 0.0
        
        max_len = max(len(s1), len(s2))
        distance = self.levenshtein_distance_optimized(s1, s2)
        return 1.0 - (distance / max_len)
    
    def parallel_levenshtein_batch(self, text_pairs: List[Tuple[str, str]]) -> List[float]:
        """
        å¹¶è¡Œè®¡ç®—å¤šä¸ªæ–‡æœ¬å¯¹çš„Levenshteinç›¸ä¼¼åº¦
        
        Args:
            text_pairs: æ–‡æœ¬å¯¹åˆ—è¡¨
            
        Returns:
            List[float]: ç›¸ä¼¼åº¦åˆ†æ•°åˆ—è¡¨
        """
        if not text_pairs:
            return []
        
        # ä½¿ç”¨GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_gpu and cp is not None:
            return self._gpu_levenshtein_batch(text_pairs)
        
        # CPUå¤šçº¿ç¨‹å¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                executor.submit(self.levenshtein_similarity, s1, s2)
                for s1, s2 in text_pairs
            ]
            results = [future.result() for future in as_completed(futures)]
        
        return results
    
    def _gpu_levenshtein_batch(self, text_pairs: List[Tuple[str, str]]) -> List[float]:
        """
        GPUåŠ é€Ÿçš„æ‰¹é‡Levenshteinè®¡ç®—
        """
        try:
            # å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—æ•°ç»„è¿›è¡ŒGPUè®¡ç®—
            similarities = []
            batch_size = 32  # GPUæ‰¹å¤„ç†å¤§å°
            
            for i in range(0, len(text_pairs), batch_size):
                batch = text_pairs[i:i + batch_size]
                batch_results = []
                
                for s1, s2 in batch:
                    # ç®€åŒ–ç‰ˆGPUè®¡ç®—ï¼Œå®é™…å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–
                    similarity = self.levenshtein_similarity(s1, s2)
                    batch_results.append(similarity)
                
                similarities.extend(batch_results)
            
            logger.debug(f"GPUå¤„ç†äº† {len(text_pairs)} ä¸ªæ–‡æœ¬å¯¹")
            return similarities
            
        except Exception as e:
            logger.warning(f"GPUè®¡ç®—å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
            return self.parallel_levenshtein_batch(text_pairs)
    
    def vector_similarity_validation(self, text1: str, text2: str) -> Tuple[bool, float]:
        """
        åŸºäºå‘é‡çš„è¯­ä¹‰ç›¸ä¼¼åº¦éªŒè¯
        
        Args:
            text1, text2: è¦æ¯”è¾ƒçš„ä¸¤ä¸ªæ–‡æœ¬
            
        Returns:
            Tuple[bool, float]: (æ˜¯å¦é€šè¿‡éªŒè¯, ç›¸ä¼¼åº¦åˆ†æ•°)
        """
        if not self.client or not self.embedding_model_name:
            logger.warning("å‘é‡æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å‘é‡ç›¸ä¼¼åº¦éªŒè¯")
            return True, 0.0
        
        try:
            # ç¡®ä¿æ–‡æœ¬æ˜¯å­—ç¬¦ä¸²ä¸”ä¸ä¸ºç©º
            if not isinstance(text1, str) or not isinstance(text2, str):
                logger.warning(f"æ–‡æœ¬ç±»å‹é”™è¯¯: text1={type(text1)}, text2={type(text2)}")
                return True, 0.0
            
            if not text1.strip() or not text2.strip():
                logger.warning("æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡å‘é‡ç›¸ä¼¼åº¦éªŒè¯")
                return True, 0.0
            
            # å‡†å¤‡æ–‡æœ¬ï¼Œç¡®ä¿æ²¡æœ‰ç‰¹æ®Šå­—ç¬¦é—®é¢˜
            clean_text1 = str(text1).strip().replace('\n', ' ').replace('\r', ' ')
            clean_text2 = str(text2).strip().replace('\n', ' ').replace('\r', ' ')
            
            # æ£€æŸ¥æ¸…ç†åçš„æ–‡æœ¬
            if not clean_text1 or not clean_text2:
                logger.warning("æ¸…ç†åæ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡å‘é‡ç›¸ä¼¼åº¦éªŒè¯")
                return True, 0.0
            
            # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…è¿‡é•¿æ–‡æœ¬å¯¼è‡´APIé”™è¯¯
            max_length = 2000
            if len(clean_text1) > max_length:
                clean_text1 = clean_text1[:max_length]
            if len(clean_text2) > max_length:
                clean_text2 = clean_text2[:max_length]
            
            # è®¡ç®—æ–‡æœ¬embeddings
            texts = [clean_text1, clean_text2]
            logger.debug(f"å‡†å¤‡è®¡ç®—å‘é‡ç›¸ä¼¼åº¦ï¼Œæ–‡æœ¬é•¿åº¦: {len(clean_text1)}, {len(clean_text2)}")
            
            # ä½¿ç”¨OpenAI clientç›´æ¥è°ƒç”¨
            response = self.client.embeddings.create(
                model=self.embedding_model_name,
                input=texts,
                dimensions=1024,
                encoding_format="float"
            )
            
            embeddings = [item.embedding for item in response.data]
            vec1 = np.array(embeddings[0])
            vec2 = np.array(embeddings[1])
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            if self.use_gpu and cp is not None:
                # GPUåŠ é€Ÿè®¡ç®—
                vec1_gpu = cp.asarray(vec1)
                vec2_gpu = cp.asarray(vec2)
                
                dot_product = cp.dot(vec1_gpu, vec2_gpu)
                norm1 = cp.linalg.norm(vec1_gpu)
                norm2 = cp.linalg.norm(vec2_gpu)
                similarity = float(dot_product / (norm1 * norm2))
            else:
                # CPUè®¡ç®—
                dot_product = np.dot(vec1, vec2)
                norm1 = np.linalg.norm(vec1)
                norm2 = np.linalg.norm(vec2)
                similarity = dot_product / (norm1 * norm2)
            
            is_valid = similarity >= self.vector_threshold
            return is_valid, similarity
            
        except Exception as e:
            logger.warning(f"å‘é‡ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return True, 0.0  # è®¡ç®—å¤±è´¥æ—¶ä¸é˜»æ­¢éªŒè¯
    
    def validate_results(self, document_data_list: List[DocumentData], 
                        detected_results: List[DuplicateOutput]) -> List[DuplicateOutput]:
        """
        ä½¿ç”¨ä¼˜åŒ–ç®—æ³•éªŒè¯æ£€æµ‹ç»“æœ
        é›†æˆBoyer-Mooreç²¾ç¡®æŸ¥æ‰¾ã€Levenshteinç¼–è¾‘è·ç¦»ã€å‘é‡ç›¸ä¼¼åº¦éªŒè¯
        """
        if not detected_results:
            return detected_results
        
        logger.info(f"ğŸ” å¼€å§‹ä¼˜åŒ–éªŒè¯æ£€æµ‹ç»“æœï¼ˆå…± {len(detected_results)} å¯¹ï¼‰...")
        
        # åˆ›å»ºæ–‡æ¡£å†…å®¹æŸ¥æ‰¾å­—å…¸
        doc_dict = {doc.document_id: doc.content for doc in document_data_list}
        
        # å‡†å¤‡æ‰¹é‡éªŒè¯æ•°æ®
        validation_data = []
        for result in detected_results:
            doc1_content = doc_dict.get(result.documentId1, "")
            doc2_content = doc_dict.get(result.documentId2, "")
            
            if doc1_content and doc2_content:
                validation_data.append({
                    'result': result,
                    'doc1_content': doc1_content,
                    'doc2_content': doc2_content
                })
        
        if not validation_data:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æ¡£å†…å®¹")
            return []
        
        # å¹¶è¡ŒéªŒè¯å¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [
                executor.submit(self._validate_single_result, data)
                for data in validation_data
            ]
            
            validated_results = []
            for future in as_completed(futures):
                result = future.result()
                if result:
                    validated_results.append(result)
        
        logger.info(f"âœ… ä¼˜åŒ–éªŒè¯å®Œæˆï¼Œä¿ç•™ {len(validated_results)}/{len(detected_results)} å¯¹é‡å¤å†…å®¹")
        return validated_results
    
    def _validate_single_result(self, data: Dict[str, Any]) -> Optional[DuplicateOutput]:
        """
        éªŒè¯å•ä¸ªæ£€æµ‹ç»“æœ
        ä½¿ç”¨ä¸‰å±‚éªŒè¯ï¼šBoyer-Mooreç²¾ç¡®åŒ¹é… â†’ Levenshteinç›¸ä¼¼åº¦ â†’ å‘é‡è¯­ä¹‰ç›¸ä¼¼åº¦
        """
        result = data['result']
        doc1_content = data['doc1_content']
        doc2_content = data['doc2_content']
        
        # ç¬¬ä¸€å±‚ï¼šBoyer-Mooreç²¾ç¡®åŒ¹é…éªŒè¯
        content1_exact_matches = self.boyer_moore_search(result.content1, doc1_content)
        content2_exact_matches = self.boyer_moore_search(result.content2, doc2_content)
        
        # å¦‚æœæ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä¼˜å…ˆä½¿ç”¨
        if content1_exact_matches and content2_exact_matches:
            logger.debug(f"âœ… ç²¾ç¡®åŒ¹é…éªŒè¯é€šè¿‡: {result.documentId1} - {result.documentId2}")
            return self._create_validated_result(result, result.content1, result.content2)
        
        # ç¬¬äºŒå±‚ï¼šLevenshteinç¼–è¾‘è·ç¦»éªŒè¯
        content1_match = self._find_best_match_levenshtein(result.content1, doc1_content)
        content2_match = self._find_best_match_levenshtein(result.content2, doc2_content)
        
        if (content1_match['similarity'] < self.similarity_threshold or 
            content2_match['similarity'] < self.similarity_threshold):
            logger.debug(f"âŒ LevenshteinéªŒè¯å¤±è´¥: content1({content1_match['similarity']:.3f}) content2({content2_match['similarity']:.3f})")
            return None
        
        # ç¬¬ä¸‰å±‚ï¼šå‘é‡è¯­ä¹‰ç›¸ä¼¼åº¦éªŒè¯
        vector_valid, vector_similarity = self.vector_similarity_validation(
            content1_match['matched_text'], 
            content2_match['matched_text']
        )
        
        if not vector_valid:
            logger.debug(f"âŒ å‘é‡ç›¸ä¼¼åº¦éªŒè¯å¤±è´¥: {vector_similarity:.3f} < {self.vector_threshold}")
            return None
        
        # æ‰€æœ‰éªŒè¯é€šè¿‡
        logger.debug(f"âœ… ä¸‰å±‚éªŒè¯å…¨éƒ¨é€šè¿‡: Levenshtein({content1_match['similarity']:.3f}, {content2_match['similarity']:.3f}) Vector({vector_similarity:.3f})")
        
        return self._create_validated_result(
            result, 
            content1_match['matched_text'], 
            content2_match['matched_text']
        )
    
    def _find_best_match_levenshtein(self, content: str, source_document: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨ä¼˜åŒ–çš„Levenshteinç®—æ³•åœ¨æºæ–‡æ¡£ä¸­æ‰¾åˆ°æœ€ä½³åŒ¹é…
        """
        content = content.strip()
        
        # ç²¾ç¡®åŒ¹é…ä¼˜å…ˆ
        if content in source_document:
            return {
                'matched_text': content,
                'similarity': 1.0,
                'method': 'exact_match'
            }
        
        best_match = {
            'matched_text': content,
            'similarity': 0.0,
            'method': 'levenshtein_match'
        }
        
        # å°†æ–‡æ¡£åˆ†å‰²æˆå¥å­/æ®µè½
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n\.!?;]+', source_document)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        # å¹¶è¡Œè®¡ç®—æ‰€æœ‰å¥å­çš„ç›¸ä¼¼åº¦
        text_pairs = [(content, sentence) for sentence in sentences]
        similarities = self.parallel_levenshtein_batch(text_pairs)
        
        # æ‰¾åˆ°æœ€ä½³åŒ¹é…
        for sentence, similarity in zip(sentences, similarities):
            if similarity > best_match['similarity']:
                best_match = {
                    'matched_text': sentence,
                    'similarity': similarity,
                    'method': 'levenshtein_sentence_match'
                }
        
        # å¦‚æœå¥å­åŒ¹é…æ•ˆæœä¸å¥½ï¼Œå°è¯•æ»‘åŠ¨çª—å£
        if best_match['similarity'] < 0.6:
            window_match = self._sliding_window_match(content, source_document)
            if window_match['similarity'] > best_match['similarity']:
                best_match = window_match
        
        return best_match
    
    def _sliding_window_match(self, content: str, source_document: str) -> Dict[str, Any]:
        """
        æ»‘åŠ¨çª—å£åŒ¹é…ï¼Œä½¿ç”¨å¹¶è¡Œå¤„ç†æé«˜æ•ˆç‡
        """
        window_size = len(content)
        step_size = max(1, window_size // 4)  # åŠ¨æ€æ­¥é•¿
        
        # å‡†å¤‡çª—å£æ–‡æœ¬å¯¹
        windows = []
        for i in range(0, len(source_document) - window_size + 1, step_size):
            window_text = source_document[i:i + window_size]
            windows.append(window_text)
        
        if not windows:
            return {
                'matched_text': content,
                'similarity': 0.0,
                'method': 'no_window_match'
            }
        
        # å¹¶è¡Œè®¡ç®—æ‰€æœ‰çª—å£çš„ç›¸ä¼¼åº¦
        text_pairs = [(content, window) for window in windows]
        similarities = self.parallel_levenshtein_batch(text_pairs)
        
        # æ‰¾åˆ°æœ€ä½³åŒ¹é…çª—å£
        best_idx = np.argmax(similarities)
        best_similarity = similarities[best_idx]
        
        return {
            'matched_text': windows[best_idx],
            'similarity': best_similarity,
            'method': 'sliding_window_match'
        }
    
    def _create_validated_result(self, original_result: DuplicateOutput, 
                               content1: str, content2: str) -> DuplicateOutput:
        """åˆ›å»ºéªŒè¯åçš„ç»“æœå¯¹è±¡"""
        prefix1, suffix1 = extract_prefix_suffix(content1)
        prefix2, suffix2 = extract_prefix_suffix(content2)
        
        return DuplicateOutput(
            documentId1=original_result.documentId1,
            page1=original_result.page1,
            chunkId1=original_result.chunkId1,
            content1=content1,
            prefix1=prefix1,
            suffix1=suffix1,
            documentId2=original_result.documentId2,
            page2=original_result.page2,
            chunkId2=original_result.chunkId2,
            content2=content2,
            prefix2=prefix2,
            suffix2=suffix2,
            reason=original_result.reason,
            score=original_result.score,
            category=original_result.category
        )